{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbinayaVina1/Scalable-Indexing/blob/main/_IRT_EX7_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okMLuC0FToiM",
        "outputId": "4c1ee80c-a591-4939-931c-2a96ed5a4eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.11/dist-packages (2.7.4)\n",
            "Indexing Complete!\n",
            "\n",
            "Search Results:\n",
            "\n",
            "Title: doc19.txt (Doc ID: doc19.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc17.txt (Doc ID: doc17.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc18.txt (Doc ID: doc18.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc20.txt (Doc ID: doc20.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc15.txt (Doc ID: doc15.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc14.txt (Doc ID: doc14.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc13.txt (Doc ID: doc13.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc12.txt (Doc ID: doc12.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc8.txt (Doc ID: doc8.txt)\n",
            "story : Term occurrances in this document: 2\n",
            "\n",
            "Title: doc5.txt (Doc ID: doc5.txt)\n",
            "story : Term occurrances in this document: 2\n"
          ]
        }
      ],
      "source": [
        "# do scalable indexing\n",
        "\n",
        "!pip install whoosh\n",
        "\n",
        "from whoosh.index import create_in, open_dir\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "from whoosh.qparser import QueryParser\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Custom stopwords List (Replace with your own)\n",
        "STOPWORDS = {\"the\", \"is\", \"in\", \"and\", \"to\", \"of\", \"a\", \"an\", \"on\", \"for\", \"with\"}\n",
        "\n",
        "# Simple stemming function (without NLTK)\n",
        "def simple_stem(word):\n",
        "    # A simple stemmer that removes common suffixes\n",
        "    suffixes = ['ing', 'ly', 'ed', 'es', 's']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "# Define a simple text processing function\n",
        "def preprocess_text(text):\n",
        "    # Convert to Lowercase and tokenize using regex\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    # Remove stopwords and apply stemming\n",
        "    processed_words = [simple_stem(word) for word in words if word not in STOPWORDS]\n",
        "    return processed_words\n",
        "\n",
        "# Define Whoosh schema with a numeric field for term frequency\n",
        "schema = Schema(\n",
        "    title=TEXT(stored=True),\n",
        "    content=TEXT,\n",
        "    doc_id=ID(stored=True, unique=True),\n",
        "    term_freq=TEXT(stored=True) # Add term_freq field as TEXT (or NUMERIC if you prefer)\n",
        ")\n",
        "\n",
        "# Create/Open index directory\n",
        "index_dir = \"indexdir\" # Use a relative path for portability\n",
        "if not os.path.exists(index_dir):\n",
        "    os.mkdir(index_dir)\n",
        "\n",
        "try:\n",
        "    index = open_dir(index_dir)\n",
        "except:\n",
        "    index = create_in(index_dir, schema)\n",
        "\n",
        "# Indexing Process\n",
        "# corpus_dir should be the directory containing the files to index, not a file itself.\n",
        "corpus_dir = \"/content/doc2\"  # Assuming 'ty.txt' is inside '/content/corpus'\n",
        "\n",
        "# Ensure the corpus directory exists.  If it doesn't, create it\n",
        "if not os.path.exists(corpus_dir):\n",
        "    os.makedirs(corpus_dir) # Use makedirs to create intermediate directories if needed\n",
        "\n",
        "# Create a dummy file if one does not exist\n",
        "dummy_file_path = os.path.join(corpus_dir, \"ty.txt\")\n",
        "if not os.path.exists(dummy_file_path):\n",
        "    with open(dummy_file_path, \"w\") as f:\n",
        "        f.write(\"This is a sample document for demonstration.\")\n",
        "\n",
        "with index.writer() as writer:\n",
        "    # Indexing with term frequency (TF) calculation\n",
        "    for filename in os.listdir(corpus_dir):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(corpus_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
        "                content = file.read()\n",
        "                processed_content = preprocess_text(content)\n",
        "\n",
        "                # Calculate term frequency for each term\n",
        "                term_freq = {}\n",
        "                for word in processed_content:\n",
        "                    term_freq[word] = term_freq.get(word, 0) + 1\n",
        "\n",
        "                # Add document with term frequency information\n",
        "                writer.add_document(\n",
        "                    title=filename,\n",
        "                    content=\" \".join(processed_content),\n",
        "                    doc_id=filename,\n",
        "                    term_freq=str(term_freq) # Store the term frequency dictionary as a string\n",
        "                )\n",
        "\n",
        "print(\"Indexing Complete!\")\n",
        "\n",
        "# Search Function with unique results (by doc_id) and term frequency of the query\n",
        "def search_index(query_text):\n",
        "    query_text = preprocess_text(query_text) # Apply the same preprocessing\n",
        "    searcher = index.searcher()\n",
        "    query = QueryParser(\"content\", index.schema).parse(\" \".join(query_text))\n",
        "\n",
        "    # Search the index with the query\n",
        "    results = searcher.search(query)\n",
        "\n",
        "    # Use a set to track unique documents by doc_id\n",
        "    unique_results = set()\n",
        "    print(\"\\nSearch Results:\")\n",
        "    for result in results:\n",
        "        # Get the document ID, title, and term frequency\n",
        "        doc_id = result['doc_id']\n",
        "\n",
        "        # Ensure that each document is printed only once (unique doc_id)\n",
        "        if doc_id not in unique_results:\n",
        "            unique_results.add(doc_id)\n",
        "            title = result['title']\n",
        "            term_freq = eval(result['term_freq']) # Convert string back to dict\n",
        "\n",
        "            # Calculate the term frequency of the query term in the document\n",
        "            query_term_freq = sum(term_freq.get(word, 0) for word in query_text)\n",
        "\n",
        "            print(f\"\\nTitle: {title} (Doc ID: {doc_id})\")\n",
        "            print(f\"{query_text[0]} : Term occurrances in this document: {query_term_freq}\")\n",
        "\n",
        "\n",
        "# Example: Searching for a term\n",
        "search_query = \"story\" # Replace with the actual query you want to search for\n",
        "search_index(search_query)"
      ]
    }
  ]
}